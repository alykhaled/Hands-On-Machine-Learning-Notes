\documentclass{article}

\usepackage{amsmath} 
\usepackage[a4paper, total={6in, 8in}]{geometry}
\title{Chapter 7: Ensemble Learning and Random Forests} 
\author{Aly Khaled} 
\date{\today} 

\begin{document} 
    \maketitle
    \section{What is Ensemble Learning?}
    \begin{itemize}
    \item Ensemble Learning in using more than one predictor then it aggregate the prediction of each predictor and predict the class that gets most votes. 
    \item \textbf{Ensemble Learning} works best when the predictors are \textbf{independent} form each other as possible and we can achieve this by \textbf{training them using very different algorithms}.
    \item \textbf{Soft voting} is to predict the class with the highest class probability averaged over all the individual \textbf{but this need all classifier to be able to estimate class probabilities which means that they all have a \textit{predict\_proba()} method}
    \item \textbf{Soft voting} is often achieve higher performance than \textbf{hard voting} because it gives more weight to highly confident votes
    \item When we train all the predictors we can make prediction for a new instance by simply aggregating the predictions of all predictors using:
    	\begin{itemize}
    		\item \textbf{Statistical mode} for classification
    		\item \textbf{Average} for regression 
    	\end{itemize}
    \item Each individual predictor has a higher bias than if it were trained on the original training set but \textbf{aggregation} reduces both bias and variance and the \textbf{net result} is that the ensemble has a similar bias but a lower variance than a single predictors trained on the original training set
    \end{itemize}
   
    \section{Bagging VS Pasting} 
    \begin{itemize}
    	\item There are two approaches for ensemble learning.
    	\begin{itemize}
    		\item \textbf{First:} Use different training algorithms.
    		\item \textbf{Second:} Use the same training algorithm for every predictor and train them on different random subsets of the training set.
    	\end{itemize}
	    \item There are two types of sampling:
	    \begin{itemize}
			\item \textbf{Bagging:} When sampling is performed with replacement.
	   		\item \textbf{Pasting:} When sampling is performed without replacement. 
	    \end{itemize}
    	\item Only \textbf{Bagging} allows training instances to be sampled several times for the same predictor .
    	\item In \textbf{Scikit\textendash Learn} the \textbf{BaggingClassifier} class automatically performs soft voting instead of hard voting if the base classifier can estimate class probabilities which means that it has a \textit{predict\_proba()} method.
    	\item \textbf{Bagging} ends up with a slightly \textbf{higher bias} than pasting; but the extra diversity also means that the predictors end up being \textbf{less correlated} so the ensemble's variance is reduced.
    	\item \textbf{Bagging} often results in better models.
    	\item Its recommended if you have spare time and CPU power to use cross validation to evaluate both of bagging and pasting and select the one that works best.
    	\item In \textbf{Scikit\textendash Learn} the \textbf{BaggingClassifier} class by default it samples \textit{m} training instances with replacement (\textit{bootstrap=True}) where \textit{m} is the size of the training set. This means only about 63\% of the training set are sampled and the remaining 37\% are called \textit{(out\textendash of\textendash bag)} instances, so we can evaluate the predictor on these instances without making validation set
    	
    	- To evaluate the predictor on these instances add (\textit{obb\_score=True}) parameter and access it through (\textit{oob\_score\_}) variable.
    	\item In \textbf{Scikit\textendash Learn} the \textbf{BaggingClassifier} class can sample features too and sampling is controlled by two hyperparameters: \textit{max\_features} and \textit{bootstrap\_features} and sampling features is useful when dealing with high-dimensional inputs (such as images).
    	\item Sampling both training instances and features is called \textbf{Random Patches Method} but keeping all training instances and sampling features is called \textbf{Random Subspaces Method}
    	
    \end{itemize}
    \section{Random Forests} 
    \begin{itemize}
		\item \textbf{Random Forests} is an ensembles of \textbf{Decision Trees} and it trained using \textbf{Bagging} method but sometimes by \textbf{Pasting}.
		\item You can make \textbf{Random Forests} by using \textbf{BaggingClassifier} class but it recommended to make it using \textbf{RandomForestClassifier} class (or by \textbf{RandomForestRegressor} class for regression) because it more convenient and optimized for \textbf{Decision Trees}.  
		\item \textbf{Random Forests} algorithm introduces extra randomness  when growing tree.
		\item Instead of searching for the very best features when splitting a node. it searches for the best features among a random subset of features. 
		\item The algorithm results in greater tree diversity and making overall better model.
		\item It is possible to make trees even more random by using random threshold for each feature rather than searching for best threshold and this tree called \textbf{Extremely Randomized Trees}.
		\item \textbf{Extremely Randomized Trees} trades more bias for a lower variance and is faster to train because searching for the best threshold is time-consuming. 
		\item We can't tell which tree forest is better between  \textbf{Extremely Randomized Trees} and \textbf{Random Forests} so its recommended to evaluate both of them and compare them using cross-validation.  
		
    \end{itemize}
	\section{Boosting}
	\begin{itemize}
		\item \textbf{Boosting} is an ensemble method that combine several weak learners to a strong learner.
		\item There are many boosting algorithms:
			\begin{itemize}
				\item AdaBoost.
				\item Gradient Tree Boosting.
				\item XGBoost.
    		\end{itemize}
    	\item The general idea of boosting is to train predictors sequentially and each trying to correct its predecessor.
   	\end{itemize}
   	\subsection{AdaBoosting}
	\begin{itemize}
    	\item \textbf{AdaBoost} technique is to pay more attention to the training instances that the predecessor underfitted
    	\item \textbf{AdaBoost} steps are:
    	\begin{enumerate}
    		\item Train a base classifier
    		\item Increase the weight of the misclassified training instances.
    		\item Train a second classifier using the updated weights.
    	\end{enumerate}
    	\item Once all predictors are trained, the ensemble makes predictions very much like bagging and pasting, except that predictors have different weights depending on their overall accuracy on the weighted training set.
    	\item One important drawback to this sequential learning technique that it cannot be \textbf{parallelized} because each predictor depends on the previous one.
    	\item \textbf{AdaBoost} algorithm is:
    	\begin{enumerate}
    		\item Each instance weight \textit{$w^{(i)}$} is initially set to \textbf{$\frac{1}{m}$}, then we compute the weighted error rate for every predictor using this equation:
    		\[ r_{j} = \frac{\underset{\hat{y}_{j}^{(i)} \neq y^{(i)}}{\sum_{i = 1}^{m} w^{(i)}}}{\sum_{i = 1}^{m} w^{(i)}}\]
    		\item Then we compute the predictor's weight $\alpha$ using this equation
    		\[ \alpha_{j} = \eta \log \frac{1 - r_{j}}{r_{j}} \]
    		\item Then update the instance weight using this equation:
    		\[ w^{(i)} = \begin{cases} 
      			w^{(i)} & \hat{y}_{j}^{(i)} = y^{(i)} \\
      			w^{(i)} exp(\alpha_{j}) & \hat{y}_{j}^{(i)} \neq y^{(i)}
   				\end{cases}
			\]
			\item Then we normalize all the instance weights by dividing by $\sum_{i = 1}^{m} w^{(i)}$
			\item Then the whole process is repeated till the number of desired predictors is reached or when a perfect predictor is found.
    	\end{enumerate}
    	\item To make prediction the \textbf{AdaBoost} simply compute the predictions of all the predictors and weighs them using the predictors weights $\alpha_{j}$, then the predicted class is the one that receives the majority of weighted votes
    	\[ \hat{y}(x) = \underset{k}{argmax} \underset{\hat{y}_{j} = k}{\sum_{j = 1}^{N} \alpha_{j}} \]
    	where N is the number of predictors.
    	\item \textbf{Scikit\textendash Learn} uses multiclass version of \textbf{AdaBoost} called \textbf{SAMME}.
    	\item When there are only two classes \textbf{SAMME} is equivalent to \textbf{AdaBoost}.
    	\item If the predictors can estimate their probabilities we can use a variant of \textbf{SAMME} called \textbf{SAMME.R} which relies on class probabilities rather than predictions and generally performs better.
    	\item \textbf{Decision Stumps} is a tree composed of as single decision node plus two leaf nodes. 
    \end{itemize}
    \subsection{Gradient Boosting}
    \begin{itemize}
    	\item \textbf{Gradient Boosting} is similar to \textbf{AdaBoost} but instead of tweaking the instance weights at every iteration like \textbf{AdaBoost} does, this method tried to fit the new predictor to the residual errors made by the previous predictor.
    	\item Using Gradient Boosting in regression is called \textbf{Gradient Tree Boosting} or \textbf{Gradient Boosted Regression Trees}.
    	\item In \textbf{Scikit\textendash Learn} you can use \textit{(GradientBoostingRegressor)} class to train \textbf{GBRT} ensembles.
    	\item The \textit{learning\_rate} hyperparameter scales the contribution of each tree. If you set it to a low value such as 0.1, you will need more trees but the prediction will generalize better and this generalization technique called \textit{shrinkage}.
    \end{itemize}
	\section{Stacking} 
	\begin{itemize}
		\item Its idea is instead of using trivial functions like hard voting to aggregate the predictions, we train a model to perform this aggregation.
		\item In stacking we take the prediction of all predictors as inputs to final predictor called \textbf{Blender}
		\item To train a \textbf{Blender}, a common approach is to use a hold-out set.
		\item To train a \textbf{Blender} first we split the training set to two subsets, then we use the first subset to train the predictors. Then the predictors are used to make predictions on the second set. We can create a new training set using these predicted values as input features and keeping the target value. The blender is trained on this new training set.
	\end{itemize}
	\section*{Papers to read later}
    \begin{itemize}
    	\item 
    \end{itemize}

\end{document}
