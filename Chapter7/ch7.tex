\documentclass{article}

\usepackage{amsmath} 
\usepackage[a4paper, total={6in, 8in}]{geometry}
\title{Chapter 7: Ensemble Learning and Random Forests} 
\author{Aly Khaled} 
\date{\today} 

\begin{document} 
    \maketitle
    \section{What is Ensemble Learning?}
    \begin{itemize}
    \item Ensemble Learning in using more than one predictor then it aggregate the prediction of each predictor and predict the class that gets most votes. 
    \item \textbf{Ensemble Learning} works best when the predictors are \textbf{independent} form each other as possible and we can achieve this by \textbf{training them using very different algorithms}.
    \item \textbf{Soft voting} is to predict the class with the highest class probability averaged over all the individual \textbf{but this need all classifier to be able to estimate class probabilities which means that they all have a \textit{predict\_proba()} method}
    \item \textbf{Soft voting} is often achieve higher performance than \textbf{hard voting} because it gives more weight to highly confident votes
    \item When we train all the predictors we can make prediction for a new instance by simply aggregating the predictions of all predictors using:
    	\begin{itemize}
    		\item \textbf{Statistical mode} for classification
    		\item \textbf{Average} for regression 
    	\end{itemize}
    \item Each individual predictor has a higher bias than if it were trained on the original training set but \textbf{aggregation} reduces both bias and variance and the \textbf{net result} is that the ensemble has a similar bias but a lower variance than a single predictors trained on the original training set
    \end{itemize}
   
    \section{Bagging VS Pasting} 
    \begin{itemize}
    	\item There are two approaches for ensemble learning.
    	\begin{itemize}
    		\item \textbf{First:} Use different training algorithms.
    		\item \textbf{Second:} Use the same training algorithm for every predictor and train them on different random subsets of the training set.
    	\end{itemize}
	    \item There are two types of sampling:
	    \begin{itemize}
			\item \textbf{Bagging:} When sampling is performed with replacement.
	   		\item \textbf{Pasting:} When sampling is performed without replacement. 
	    \end{itemize}
    	\item Only \textbf{Bagging} allows training instances to be sampled several times for the same predictor .
    	\item In \textbf{Scikit\textendash Learn} the \textbf{BaggingClassifier} class automatically performs soft voting instead of hard voting if the base classifier can estimate class probabilities which means that it has a \textit{predict\_proba()} method.
    	\item \textbf{Bagging} ends up with a slightly \textbf{higher bias} than pasting; but the extra diversity also means that the predictors end up being \textbf{less correlated} so the ensemble's variance is reduced.
    	\item \textbf{Bagging} often results in better models.
    	\item Its recommended if you have spare time and CPU power to use cross validation to evaluate both of bagging and pasting and select the one that works best.
    	\item In \textbf{Scikit\textendash Learn} the \textbf{BaggingClassifier} class by default it samples \textit{m} training instances with replacement (\textit{bootstrap=True}) where \textit{m} is the size of the training set. This means only about 63\% of the training set are sampled and the remaining 37\% are called \textit{(out\textendash of\textendash bag)} instances, so we can evaluate the predictor on these instances without making validation set
    	
    	- To evaluate the predictor on these instances add (\textit{obb\_score=True}) parameter and access it through (\textit{oob\_score\_}) variable.
    	\item In \textbf{Scikit\textendash Learn} the \textbf{BaggingClassifier} class can sample features too and sampling is controlled by two hyperparameters: \textit{max\_features} and \textit{bootstrap\_features} and sampling features is useful when dealing with high-dimensional inputs (such as images).
    	\item Sampling both training instances and features is called \textbf{Random Patches Method} but keeping all training instances and sampling features is called \textbf{Random Subspaces Method}
    	
    \end{itemize}
    \section{Random Forests} 
    \begin{itemize}
		\item \textbf{Random Forests} is an ensembles of \textbf{Decision Trees} and it trained using \textbf{Bagging} method but sometimes by \textbf{Pasting}.
		\item You can make \textbf{Random Forests} by using \textbf{BaggingClassifier} class but it recommended to make it using \textbf{RandomForestClassifier} class (or by \textbf{RandomForestRegressor} class for regression) because it more convenient and optimized for \textbf{Decision Trees}.  
		\item \textbf{Random Forests} algorithm introduces extra randomness  when growing tree.
		\item Instead of searching for the very best features when splitting a node. it searches for the best features among a random subset of features. 
		\item The algorithm results in greater tree diversity and making overall better model.
		
    \end{itemize}
	\section{Boosting} 
\end{document}
